{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will assess daily ensemble hindcast data provided by the NWS. It will read the daily .xml files, calculate the cumulative volume over the first 5 days, for each of the 60 forcasts published  each day, and report those volumes to a .csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imported Libraries\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as et\n",
    "import io\n",
    "import os\n",
    "import statistics\n",
    "import matplotlib\n",
    "from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt\n",
    "import openpyxl\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Methods\n",
    "def kcfs2acFtHour(foo):\n",
    "    return (foo*1000*3600/43560)\n",
    "\n",
    "def importForecastXML(fullFilePath):\n",
    "# Reads XML DEPENDENT ON kcfs2acFtHour\n",
    "    tree = et.parse(fullFilePath)\n",
    "    root = tree.getroot()\n",
    "    seriesCount = 1\n",
    "    data = []\n",
    "    for eachSeries in root[1:]:\n",
    "        seriesID = seriesCount\n",
    "        seriesCount = seriesCount + 1\n",
    "        for eachEvent in eachSeries[1:]:\n",
    "            eachEvent.attrib['seriesID'] = seriesID\n",
    "            data.append(eachEvent.attrib)\n",
    "    df = pd.DataFrame(data)\n",
    "    #Prep Dataframe\n",
    "    df['value'] = df['value'].astype(float)\n",
    "    df['value'] = df['value'].apply(kcfs2acFtHour)\n",
    "    df = df.set_index('seriesID')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above this establishes dependencies, and defines two methods, which convert kcfs to acre-ft per hour and import the forecastXMLs into a format we can work with in pandas. \n",
    "\n",
    "Below this establishes the list of folders containing the files we're interested in. This is can be edited depending on the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directoryList = [r\"C:\\Users\\q0hecbbb\\Projects\\Hindcast Data\\LM FVA HEMP - Forecasts\\Hindcasts\\1985-2010\\LAMC1\",\n",
    "#                 r\"C:\\Users\\q0hecbbb\\Projects\\Hindcast Data\\LM FVA HEMP - Forecasts\\Hindcasts\\2010-2017\\LAMC1\",\n",
    "#                 r\"C:\\Users\\q0hecbbb\\Projects\\Hindcast Data\\LM FVA HEMP - Forecasts\\Hindcasts\\Mar1995\\200\\LAMC1\",\n",
    "#                 r\"C:\\Users\\q0hecbbb\\Projects\\Hindcast Data\\LM FVA HEMP - Forecasts\\Hindcasts\\Mar1995\\500\\LAMC1\",\n",
    "#                  r\"C:\\Users\\q0hecbbb\\Projects\\Hindcast Data\\LM FVA HEMP - Forecasts\\Hindcasts\\Scaled 200y\\1986_200yr_hindcasts\\LAMC1\",\n",
    "#                  r\"C:\\Users\\q0hecbbb\\Projects\\Hindcast Data\\LM FVA HEMP - Forecasts\\Hindcasts\\Scaled 200y\\1997_200yr_hindcasts\\LAMC1\",\n",
    "#                  r\"C:\\Users\\q0hecbbb\\Projects\\Hindcast Data\\LM FVA HEMP - Forecasts\\Hindcasts\\Scaled 200y\\2006_200yr_hindcasts\\LAMC1\",\n",
    "#                  r\"C:\\Users\\q0hecbbb\\Projects\\Hindcast Data\\LM FVA HEMP - Forecasts\\Hindcasts\\Scaled 500y\\1986_500yr_hindcasts\\LAMC1\",\n",
    "#                  r\"C:\\Users\\q0hecbbb\\Projects\\Hindcast Data\\LM FVA HEMP - Forecasts\\Hindcasts\\Scaled 500y\\1997_500yr_hindcasts\\LAMC1\",\n",
    "#                  r\"C:\\Users\\q0hecbbb\\Projects\\Hindcast Data\\LM FVA HEMP - Forecasts\\Hindcasts\\Scaled 500y\\2006_500yr_hindcasts\\LAMC1\",]\n",
    "\n",
    "directoryList = [r\"C:\\Users\\q0hecbbb\\Projects\\Hindcast Data\\LM FVA HEMP - Forecasts\\Hindcasts\\Additional200500\\200\\LAMC1\",\n",
    "                r\"C:\\Users\\q0hecbbb\\Projects\\Hindcast Data\\LM FVA HEMP - Forecasts\\Hindcasts\\Additional200500\\500\\LAMC1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cycles through all the specified directories, performs a cumulative volume analysis, and outputs the results to a csv. \n",
    "\n",
    "The cumulative volume analysis first converts the kcfs values from the forecast .xmls into acre-ft/hrs. Each day's forecast includes 61 different forecasts, refered to as the ensemble forecast. The first hour is removed from the ensemble forecast (originally starting at 1200 gmt) because the hourly flow values represent the end of the period, rather than the begining of the period, so including the first value would mean counting flow value which has already passed. \n",
    "\n",
    "For each series in the ensemble, a 1, 2, 3, 4, and 5-day cumulative volume is computed by summing 24, 48... rows of data (hours of data). Those values are saved into a list. There is a list which contains all the 1 day volumes, 2 day volumes, etc.. Once all series have been processed, we calculate the 10,25,50,75,90th percentiles and mean for each of those lists, and report those values in a csv. The column of the csv indicates which forecast date these values come from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the process at 2020-01-28 14:25:18.615083\n",
      "There are 51 in our directory.\n",
      "starting file number: 0 at 2020-01-28 14:25:18.824806\n",
      "Done!\n",
      "Starting the process at 2020-01-28 14:25:33.681909\n",
      "There are 51 in our directory.\n",
      "starting file number: 0 at 2020-01-28 14:25:33.878980\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Main\n",
    "for eachDirectory in directoryList:\n",
    "    \n",
    "    progress_count = 0\n",
    "    masterDictionary = {\"Cum_Vol\" : ['1-day (Mean)', '2-day (Mean)', '3-day (Mean)', '4-day (Mean)', '5-day (Mean)', '1-day (10 PCTL)', '2-day (10 PCTL)', '3-day (10 PCTL)', '4-day (10 PCTL)', '5-day (10 PCTL)','1-day (25 PCTL)', '2-day (25 PCTL)', '3-day (25 PCTL)', '4-day (25 PCTL)', '5-day (25 PCTL)','1-day (50 PCTL)', '2-day (50 PCTL)', '3-day (50 PCTL)', '4-day (50 PCTL)', '5-day (50 PCTL)','1-day (75 PCTL)', '2-day (75 PCTL)', '3-day (75 PCTL)', '4-day (75 PCTL)', '5-day (75 PCTL)','1-day (90 PCTL)', '2-day (90 PCTL)', '3-day (90 PCTL)', '4-day (90 PCTL)', '5-day (90 PCTL)']}\n",
    "    currentDT = datetime.datetime.now()\n",
    "    print(\"Starting the process at {}\".format(str(currentDT)))\n",
    "    print(\"There are \" + str(len(os.listdir(eachDirectory))) + \" in our directory.\")\n",
    "    \n",
    "    for targetFile in os.listdir(eachDirectory):\n",
    "        df = importForecastXML(eachDirectory + '\\\\' + targetFile)\n",
    "        if progress_count % 500 == 0:\n",
    "            currentDT = datetime.datetime.now()\n",
    "            print(\"starting file number: \" + str(progress_count) + \" at \" + str(currentDT))\n",
    "        progress_count = progress_count + 1\n",
    "        defaultIndexDF = df.reset_index()\n",
    "        forecastDate = defaultIndexDF.date[0]\n",
    "\n",
    "        day1List = []\n",
    "        day2List = []\n",
    "        day3List = []\n",
    "        day4List = []\n",
    "        day5List = []\n",
    "        outputList = []\n",
    "\n",
    "        for num in range(1, 61):\n",
    "            hours = 24\n",
    "            currentSeries = df.loc[num]\n",
    "            currentSeries = currentSeries.iloc[1:] #This cuts off the first hour from the analysis\n",
    "\n",
    "            day1List.append(currentSeries.value.head(hours).sum(0))\n",
    "            hours = hours + 24\n",
    "            day2List.append(currentSeries.value.head(hours).sum(0))\n",
    "            hours = hours + 24\n",
    "            day3List.append(currentSeries.value.head(hours).sum(0))\n",
    "            hours = hours + 24\n",
    "            day4List.append(currentSeries.value.head(hours).sum(0))\n",
    "            hours = hours + 24\n",
    "            day5List.append(currentSeries.value.head(hours).sum(0))\n",
    "\n",
    "        listOflists = [day1List, day2List, day3List, day4List, day5List]\n",
    "        \n",
    "        for eachList in listOflists:\n",
    "            outputList.append(np.mean(eachList))\n",
    "        for eachList in listOflists:\n",
    "            outputList.append(np.percentile(eachList, 10))  \n",
    "        for eachList in listOflists:\n",
    "            outputList.append(np.percentile(eachList, 25))\n",
    "        for eachList in listOflists:\n",
    "            outputList.append(np.percentile(eachList, 50))\n",
    "        for eachList in listOflists:\n",
    "            outputList.append(np.percentile(eachList, 75))\n",
    "        for eachList in listOflists:\n",
    "            outputList.append(np.percentile(eachList, 90))\n",
    "\n",
    "        masterDictionary.update({forecastDate : outputList})\n",
    "    dfOut = pd.DataFrame(masterDictionary)\n",
    "    splitName = eachDirectory.split('\\\\') \n",
    "    dfOut.to_csv(r'C:\\Users\\q0hecbbb\\Projects\\Hindcast Data\\Batch_Out\\Hindcast_AvgCumVol_'+splitName[7]+splitName[8]+'.csv')\n",
    "    print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
